{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:47.482217Z",
     "start_time": "2020-09-02T18:48:47.245642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tjrud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from nlp import load_metric\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up WandB for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: tjrudjjang (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "## Login to WandB and get your API Key\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:47.486121Z",
     "start_time": "2020-09-02T18:48:47.483336Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-62936f4d8972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mYOUR_API_KEY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'c573cfd0aef350915afa3bae09b374990b01de4f'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"WANDB_API_KEY\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYOUR_API_KEY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mwandb_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWandbLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wikohow-t5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# wandb.init(project=\"transformers_tutorials_summarization\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, save_dir, offline, id, anonymous, version, project, log_model, experiment, prefix, sync_step, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     ):\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwandb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             raise ImportError(\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;34m'You want to use `wandb` logger which is not installed yet,'\u001b[0m  \u001b[1;31m# pragma: no-cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m' install it with `pip install wandb`.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`."
     ]
    }
   ],
   "source": [
    "## Paste your API key in the YOUR_API_KEY variable below\n",
    "import wandb\n",
    "YOUR_API_KEY = 'c573cfd0aef350915afa3bae09b374990b01de4f'\n",
    "os.environ[\"WANDB_API_KEY\"] = YOUR_API_KEY\n",
    "wandb_logger = WandbLogger(project='wikohow-t5')\n",
    "# wandb.init(project=\"transformers_tutorials_summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data using NLP Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:47.865252Z",
     "start_time": "2020-09-02T18:48:47.487297Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlp import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "print(', '.join(dataset.id for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select WikiHow data set\n",
    "See more about it at - https://www.tensorflow.org/datasets/catalog/wikihow\n",
    "\n",
    "Manual Download required - Download wikihowAll.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.253885Z",
     "start_time": "2020-09-02T18:48:47.868015Z"
    }
   },
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('wikihow', 'all', data_dir='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.257189Z",
     "start_time": "2020-09-02T18:48:48.255049Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.263225Z",
     "start_time": "2020-09-02T18:48:48.258232Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Size of train dataset: \", dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.269900Z",
     "start_time": "2020-09-02T18:48:48.264319Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Size of Validation dataset: \", dataset['validation'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the test data set for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.274748Z",
     "start_time": "2020-09-02T18:48:48.271169Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Size of ca test dataset: \", dataset['test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Examples in this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.279408Z",
     "start_time": "2020-09-02T18:48:48.275717Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:48:48.284043Z",
     "start_time": "2020-09-02T18:48:48.280221Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" Example of text: \", dataset['train'][2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:38.775524Z",
     "start_time": "2020-09-01T13:44:38.771923Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" Example of Summary: \", dataset['train'][0]['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:38.780418Z",
     "start_time": "2020-09-01T13:44:38.776344Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" Example of Title: \", dataset['train'][0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate average length of Text and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:38.785557Z",
     "start_time": "2020-09-01T13:44:38.781468Z"
    }
   },
   "outputs": [],
   "source": [
    "tiny_dataset = dataset['train'].select(list(range(0, 100)))\n",
    "text_len = []\n",
    "summary_len=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:38.801235Z",
     "start_time": "2020-09-01T13:44:38.786503Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(tiny_dataset)):\n",
    "    example = tiny_dataset[i]\n",
    "    text_example = example['text']\n",
    "    text_example = text_example.replace('\\n','')\n",
    "    text_words = text_example.split()\n",
    "    text_len.append(len(text_words))\n",
    "    summary_example = example['headline']\n",
    "    summary_example = summary_example.replace('\\n','')\n",
    "    summary_words = summary_example.split()\n",
    "    summary_len.append(len(summary_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:46:50.185114Z",
     "start_time": "2020-09-02T18:46:50.047191Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(text_len)\n",
    "plt.title('Text Length Distribution - First 100 examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:47:08.641831Z",
     "start_time": "2020-09-02T18:47:08.524395Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(summary_len)\n",
    "plt.title('Summary Length Distribution - First 100 examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.138817Z",
     "start_time": "2020-09-01T13:44:39.136486Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Length of text: \", sum(text_len)/len(text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.143573Z",
     "start_time": "2020-09-01T13:44:39.139797Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Length of Summary: \", sum(summary_len)/len(summary_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.162456Z",
     "start_time": "2020-09-01T13:44:39.144570Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.185299Z",
     "start_time": "2020-09-01T13:44:39.163413Z"
    }
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams        \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "        self.rouge_metric = load_metric('rouge') \n",
    "        \n",
    "        if self.hparams.freeze_embeds:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams.freeze_encoder:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "            \n",
    "        n_observations_per_split = {\n",
    "            \"train\": self.hparams.n_train,\n",
    "            \"validation\": self.hparams.n_val,\n",
    "            \"test\": self.hparams.n_test,\n",
    "        }\n",
    "        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n",
    "        \n",
    "    \n",
    "    def freeze_params(self, model):\n",
    "        for par in model.parameters():\n",
    "            par.requires_grad = False\n",
    "            \n",
    "            \n",
    "    def freeze_embeds(self):\n",
    "        \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
    "        try:\n",
    "            self.freeze_params(self.model.model.shared)\n",
    "            for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "                freeze_params(d.embed_positions)\n",
    "                freeze_params(d.embed_tokens)\n",
    "        except AttributeError:\n",
    "            self.freeze_params(self.model.shared)\n",
    "            for d in [self.model.encoder, self.model.decoder]:\n",
    "                self.freeze_params(d.embed_tokens)\n",
    "    \n",
    "    def lmap(self, f, x):\n",
    "        \"\"\"list(map(f, x))\"\"\"\n",
    "        return list(map(f, x))\n",
    "    \n",
    "\n",
    "    def is_logger(self):\n",
    "        return self.trainer.proc_rank <= 0\n",
    "    \n",
    "    \n",
    "    def parse_score(self, result):\n",
    "        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
    "        \n",
    "    def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "  ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def ids_to_clean_text(self, generated_ids):\n",
    "        gen_text = self.tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return self.lmap(str.strip, gen_text)\n",
    "    \n",
    "    \n",
    "    def _generative_step(self, batch) :\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            use_cache=True,\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        preds = self.ids_to_clean_text(generated_ids)\n",
    "        target = self.ids_to_clean_text(batch[\"target_ids\"])\n",
    "            \n",
    "        gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n",
    "    \n",
    "        loss = self._step(batch)\n",
    "        base_metrics = {'val_loss': loss}\n",
    "#         rouge: Dict = self.calc_generative_metrics(preds, target)\n",
    "        summ_len = np.mean(self.lmap(len, generated_ids))\n",
    "        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n",
    "        self.rouge_metric.add_batch(preds, target)\n",
    "        \n",
    "#         rouge_results = self.rouge_metric.compute() \n",
    "#         rouge_dict = self.parse_score(rouge_results)\n",
    "#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
    "        \n",
    "        return base_metrics\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "  \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._generative_step(batch)\n",
    "    \n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        \n",
    "        rouge_results = self.rouge_metric.compute() \n",
    "        rouge_dict = self.parse_score(rouge_results)\n",
    "    \n",
    "        tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
    "        \n",
    "        ## Clear out the lists for next epoch\n",
    "        self.target_gen= []\n",
    "        self.prediction_gen=[]\n",
    "        return {\"avg_val_loss\": avg_loss, \n",
    "                \"rouge1\" : rouge_results['rouge1'],\n",
    "                \"rougeL\" : rouge_results['rougeL'],\n",
    "                \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "  \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=False):\n",
    "        if self.trainer.use_tpu:\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "  \n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "    \n",
    "\n",
    "    def train_dataloader(self):   \n",
    "        n_samples = self.n_obs['train']\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", num_samples=n_samples, args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "            // self.hparams.gradient_accumulation_steps\n",
    "            * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        n_samples = self.n_obs['validation']\n",
    "        validation_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"validation\", num_samples=n_samples, args=self.hparams)\n",
    "        \n",
    "        return DataLoader(validation_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        n_samples = self.n_obs['test']\n",
    "        test_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"test\", num_samples=n_samples, args=self.hparams)\n",
    "        \n",
    "        return DataLoader(test_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.192404Z",
     "start_time": "2020-09-01T13:44:39.186281Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            # Log results\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "\n",
    "            # Log and save results to file\n",
    "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a DataSet class for the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.203467Z",
     "start_time": "2020-09-01T13:44:39.193447Z"
    }
   },
   "outputs": [],
   "source": [
    "class wikihow(Dataset):\n",
    "    def __init__(self, tokenizer, type_path, num_samples, input_length, output_length, print_text=False):         \n",
    "        self.dataset =  load_dataset('wikihow', 'all', data_dir='data/', split=type_path)\n",
    "        if num_samples:\n",
    "            self.dataset = self.dataset.select(list(range(0, num_samples)))\n",
    "        self.input_length = input_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_length = output_length\n",
    "        self.print_text = print_text\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.replace('Example of text:', '')\n",
    "        text = text.replace('Example of Summary:', '')\n",
    "        text = text.replace('\\n','')\n",
    "        text = text.replace('``', '')\n",
    "        text = text.replace('\"', '')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def convert_to_features(self, example_batch):\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "        \n",
    "        if self.print_text:\n",
    "            print(\"Input Text: \", self.clean_text(example_batch['text']))\n",
    "#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n",
    "#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n",
    "        \n",
    "        input_ = self.clean_text(example_batch['text'])\n",
    "        target_ = self.clean_text(example_batch['headline'])\n",
    "        \n",
    "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "       \n",
    "        return source, targets\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        source, targets = self.convert_to_features(self.dataset[index])\n",
    "        \n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask    = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = targets[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:55:41.647100Z",
     "start_time": "2020-09-02T18:55:41.023703Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "dataset = wikihow(tokenizer, 'validation', None, 512, 150, True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T18:55:43.381341Z",
     "start_time": "2020-09-02T18:55:43.366366Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset[50]\n",
    "print()\n",
    "print(\"Shape of Tokenized Text: \", data['source_ids'].shape)\n",
    "print()\n",
    "print(\"Sanity check - Decode Text: \", tokenizer.decode(data['source_ids']))\n",
    "print(\"====================================\")\n",
    "print(\"Sanity check - Decode Summary: \", tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.830252Z",
     "start_time": "2020-09-01T13:44:39.824446Z"
    }
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-small',\n",
    "    tokenizer_name_or_path='t5-small',\n",
    "    max_input_length=512,\n",
    "    max_output_length=150,\n",
    "    freeze_encoder=False,\n",
    "    freeze_embeds=False,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=4,\n",
    "    eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    n_gpu=1,\n",
    "    resume_from_checkpoint=None, \n",
    "    val_check_interval = 0.05, \n",
    "    n_val=1000,\n",
    "    n_train=-1,\n",
    "    n_test=-1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:28:40.222324Z",
     "start_time": "2020-09-01T13:28:40.103545Z"
    }
   },
   "source": [
    "!mkdir -p t5_wikihow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.836856Z",
     "start_time": "2020-09-01T13:44:39.831154Z"
    }
   },
   "outputs": [],
   "source": [
    "args_dict.update({'output_dir': 't5_wikihow', 'num_train_epochs':2,\n",
    "                 'train_batch_size': 4, 'eval_batch_size': 4})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.843602Z",
     "start_time": "2020-09-01T13:44:39.837733Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
    ")\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    val_check_interval=args.val_check_interval,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:39.853037Z",
     "start_time": "2020-09-01T13:44:39.844578Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, num_samples, args):\n",
    "      return wikihow(tokenizer=tokenizer, type_path=type_path, num_samples=num_samples,  input_length=args.max_input_length, \n",
    "                        output_length=args.max_output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:42.048444Z",
     "start_time": "2020-09-01T13:44:39.854009Z"
    }
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:44:42.072260Z",
     "start_time": "2020-09-01T13:44:42.053193Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T20:25:25.960117Z",
     "start_time": "2020-09-01T13:44:42.076449Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T00:59:41.863404Z",
     "start_time": "2020-09-02T00:59:41.859895Z"
    }
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T01:00:09.047610Z",
     "start_time": "2020-09-02T01:00:08.417634Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "dataset = wikihow(tokenizer, 'test', None, 512, 150, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T01:00:29.151446Z",
     "start_time": "2020-09-02T01:00:29.143943Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T01:00:34.602089Z",
     "start_time": "2020-09-02T01:00:34.443983Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T01:02:59.809227Z",
     "start_time": "2020-09-02T01:02:51.326688Z"
    }
   },
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "outs = model.model.generate(\n",
    "            batch[\"source_ids\"].cuda(),\n",
    "            attention_mask=batch[\"source_mask\"].cuda(),\n",
    "            use_cache=True,\n",
    "            decoder_attention_mask=batch['target_mask'].cuda(),\n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "dec = [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T01:03:57.193269Z",
     "start_time": "2020-09-02T01:03:57.138546Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    lines = textwrap.wrap(\"WikiHow Text:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual Summary: %s\" % targets[i])\n",
    "    print(\"\\nPredicted Summary: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using AutoModel loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:08.098275Z",
     "start_time": "2020-09-06T12:11:08.096343Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:12.532641Z",
     "start_time": "2020-09-06T12:11:09.708535Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deep-learning-analytics/wikihow-t5-small\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"deep-learning-analytics/wikihow-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:14.162773Z",
     "start_time": "2020-09-06T12:11:12.533612Z"
    }
   },
   "outputs": [],
   "source": [
    "## Move to CUDA\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:19.675892Z",
     "start_time": "2020-09-06T12:11:19.670482Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "Lack of fluids can lead to dry mouth, which is a leading cause of bad breath. Water\n",
    "can also dilute any chemicals in your mouth or gut that are causing bad breath., Studies show that\n",
    "eating 6 ounces of yogurt a day reduces the level of odor-causing compounds in the mouth. In\n",
    "particular, look for yogurt containing the active bacteria Streptococcus thermophilus or\n",
    "Lactobacillus bulgaricus., The abrasive nature of fibrous fruits and vegetables helps to clean\n",
    "teeth, while the vitamins, antioxidants, and acids they contain improve dental health.Foods that can\n",
    "be particularly helpful include:Apples — Apples contain vitamin C, which is necessary for health\n",
    "gums, as well as malic acid, which helps to whiten teeth.Carrots — Carrots are rich in vitamin A,\n",
    "which strengthens tooth enamel.Celery — Chewing celery produces a lot of saliva, which helps to\n",
    "neutralize bacteria that cause bad breath.Pineapples — Pineapples contain bromelain, an enzyme that\n",
    "cleans the mouth., These teas have been shown to kill the bacteria that cause bad breath and\n",
    "plaque., An upset stomach can lead to burping, which contributes to bad breath. Don’t eat foods that\n",
    "upset your stomach, or if you do, use antacids. If you are lactose intolerant, try lactase tablets.,\n",
    "They can all cause bad breath. If you do eat them, bring sugar-free gum or a toothbrush and\n",
    "toothpaste to freshen your mouth afterwards., Diets low in carbohydrates lead to ketosis — a state\n",
    "in which the body burns primarily fat instead of carbohydrates for energy. This may be good for your\n",
    "waistline, but it also produces chemicals called ketones, which contribute to bad breath.To stop the\n",
    "problem, you must change your diet. Or, you can combat the smell in one of these ways:Drink lots of\n",
    "water to dilute the ketones.Chew sugarless gum or suck on sugarless mints.Chew mint leaves.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:25.940815Z",
     "start_time": "2020-09-06T12:11:25.937188Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:32.501144Z",
     "start_time": "2020-09-06T12:11:31.917512Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_ids = model.generate(\n",
    "            tokenized_text,\n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T12:11:38.309701Z",
     "start_time": "2020-09-06T12:11:38.289453Z"
    }
   },
   "outputs": [],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
